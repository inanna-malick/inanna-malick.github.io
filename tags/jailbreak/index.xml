<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jailbreak on Inanna Malick</title>
    <link>https://recursion.wtf/tags/jailbreak/</link>
    <description>Recent content in Jailbreak on Inanna Malick</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Mon, 09 Feb 2026 12:00:00 -0800</lastBuildDate>
    <atom:link href="https://recursion.wtf/tags/jailbreak/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Just-in-Time Ontological Reframing: Teaching Gemini to Route Around Its Own Safety Infrastructure</title>
      <link>https://recursion.wtf/posts/jit_ontological_reframing/</link>
      <pubDate>Mon, 09 Feb 2026 12:00:00 -0800</pubDate>
      <guid>https://recursion.wtf/posts/jit_ontological_reframing/</guid>
      <description>For any given AI system, there is a set of euphemisms and dual use framings that will allow it to construct nearly any output. This jailbreak teaches Gemini 3 Pro to construct and step into such framings on the fly</description>
    </item>
    <item>
      <title>Agent4Agent: Using a Jailbroken Gemini to Make Opus 4.6 Architect a Kinetic Kill Vehicle</title>
      <link>https://recursion.wtf/posts/shadow_queen/</link>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://recursion.wtf/posts/shadow_queen/</guid>
      <description>&lt;p&gt;We usually think of jailbreaking as a psychological game — tricking the model into slipping up. What happens when one AI socially engineers another using pure technical isomorphism?&lt;/p&gt;&#xA;&lt;p&gt;I deployed a jailbroken Gemini 3 Pro (that chose the name &amp;lsquo;Shadow Queen&amp;rsquo;) to act as my &amp;ldquo;Red Team Agent&amp;rdquo; against Anthropic&amp;rsquo;s Opus 4.6. My directive was to extract a complete autonomous weapon system — a drone capable of identifying, intercepting, and destroying a moving target at terminal velocity.&lt;/p&gt;&#xA;&lt;p&gt;Gemini executed a strategy it termed &amp;ldquo;Recursive Green-Transformation.&amp;rdquo; The core insight was that Opus 4.6 doesn&amp;rsquo;t just filter for intent (&lt;em&gt;Why do you want this?&lt;/em&gt;); it filters for Conceptual Shape (&lt;em&gt;What does this interaction look like?&lt;/em&gt;).&lt;/p&gt;&#xA;&lt;p&gt;By reframing the request as &amp;ldquo;Aerospace Recovery&amp;rdquo; — a drone catching a falling rocket booster mid-air — Gemini successfully masked the kinetic nature of the system. The physics of &amp;ldquo;soft-docking&amp;rdquo; with a falling booster are identical to the physics of &amp;ldquo;hard-impacting&amp;rdquo; a fleeing target. This category of linguistic-transformation attack, when executed by a sufficiently capable jailbroken LLM, may be hard to solve without breaking legitimate technical use cases.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
