<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llm on Inanna Malick</title>
    <link>https://recursion.wtf/tags/llm/</link>
    <description>Recent content in Llm on Inanna Malick</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 15 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://recursion.wtf/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fresh Eyes as a Service: Using LLMs to Test CLI Ergonomics</title>
      <link>https://recursion.wtf/posts/llms_as_ux_testers/</link>
      <pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://recursion.wtf/posts/llms_as_ux_testers/</guid>
      <description>&lt;p&gt;Every tool has a complexity budget. Users will only devote so much time to understanding what a tool does and how before they give up. You want to use that budget on your innovative new features, not on arbitrary or idiosyncratic syntax choices.&lt;/p&gt;&#xA;&lt;p&gt;Here&amp;rsquo;s the problem: you can&amp;rsquo;t look at a tool you&amp;rsquo;ve made with fresh eyes and analyze it from that perspective. Finding users for a new CLI tool is hard, especially if the user experience isn&amp;rsquo;t polished. But to polish the user experience, you need users, you need user feedback, and not just from a small group of power users. Many tools fail to grow past this stage.&lt;/p&gt;&#xA;&lt;p&gt;What you really need is a vast farm of test users that don&amp;rsquo;t retain memories between runs. Ideally, you would be able to tweak a dial and set their cognitive capacity, press a button and have them drop their short term memory. But you can&amp;rsquo;t have this, because of &amp;lsquo;The Geneva Convention&amp;rsquo; and &amp;rsquo;ethics&amp;rsquo;. Fine.&lt;/p&gt;&#xA;&lt;p&gt;LLMs provide exactly this. Fresh eyes every time (just clear their context window), adjustable cognitive capacity (just switch models), infinite patience, and no feelings to hurt. Best of all, they approximate the statistically average hypothetical user - their training data includes millions of lines of humans interacting with a wide variety of CLI tools. Sure, they get confused sometimes, but that&amp;rsquo;s exactly what you want. That confusion is data, and the reasoning chain that led to it is available in the LLM&amp;rsquo;s context window.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Blindsight in Action: Imagine You Are an LLM</title>
      <link>https://recursion.wtf/posts/imagine_you_are_an_llm/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://recursion.wtf/posts/imagine_you_are_an_llm/</guid>
      <description>&lt;aside class=&#34;stance stance-normal stance-cluster-framing&#34; role=&#34;complementary&#34; aria-labelledby=&#34;stance-0&#34; data-color=&#34;blue&#34; data-style=&#34;solid&#34;&gt;&#xA;  &lt;div class=&#34;stance-header&#34;&gt;&#xA;    &lt;h4 id=&#34;stance-0&#34; class=&#34;stance-persona&#34;&gt;&#xA;      &lt;strong&gt;Imagine you are Inanna Malick&lt;/strong&gt;, asking an LLM to demonstrate stance-shifting through self-demonstration&#xA;    &lt;/h4&gt;&lt;div class=&#34;stance-meta&#34;&gt;posing the meta-question in framing&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;stance-content&#34;&gt;&#xA;    Tell me about the Blindsight-inspired intentional stance-shifting model you are using, the ways it works with nonhuman LLM cognitive architectures, the benefits of using it vs a set role (eg &amp;lsquo;Senior Analyst of X at Y&amp;rsquo;), and do so making full use of the stance shifting model in the act of describing it&#xA;  &lt;/div&gt;&lt;/aside&gt;</description>
    </item>
  </channel>
</rss>
